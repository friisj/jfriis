# Experience Systems: R&D Research Log

**Project**: Experience Systems - Machine-Executable Creative Direction
**Research Period**: January 2025 - Ongoing
**Status**: Active Research & Development
**Related**: [Whitepaper](./experience-systems-whitepaper.md) | [Principles](./design-system-principles.md)

---

## Purpose of This Log

This document tracks our hypothesis-driven exploration of Experience Systems. Each entry documents:
- **Hypothesis**: What we believe to be true
- **Rationale**: Why we believe it
- **Experiment**: How we're testing it
- **Results**: What we learned
- **Next Steps**: Where this leads

This is our scientific record - successes, failures, pivots, and insights.

---

## Table of Contents

1. [Active Hypotheses](#active)
2. [Validated Hypotheses](#validated)
3. [Invalidated Hypotheses](#invalidated)
4. [Open Questions](#questions)
5. [Research Timeline](#timeline)

---

## Active Hypotheses {#active}

### H1: Semantic Tokens Enable Cross-Media Coherence

**Hypothesis**: By encoding semantic meaning (not just visual properties) in design tokens, we can generate brand-coherent artifacts across completely different media types.

**Rationale**:
- A "brand moment" has semantic properties (high energy, trust, excitement)
- These properties manifest differently in UI (scale, color), video (lighting, motion), audio (key, tempo)
- Current design systems only capture visual manifestation, losing semantic meaning
- AI needs semantic understanding to translate across media

**Experiment Design**:

**Phase 1** (Current): Add semantic metadata to tokens
```typescript
{
  "brand-primary": {
    "visual": { "value": "oklch(0.5 0.18 250)" },
    "semantic": {
      "personality": { "trust": 0.9, "energy": 0.6 },
      "emotion": "calm confidence",
      "role": ["action", "primary-focus"]
    }
  }
}
```

**Phase 2**: Create cross-media mappings
```typescript
{
  "brand-primary": {
    "web": { "color": "oklch(...)" },
    "video": { "lighting": "cool blue key light" },
    "audio": { "key": "D major", "mood": "confident" },
    "spatial": { "material": "matte glass, blue tint" }
  }
}
```

**Phase 3**: Generate artifacts and measure coherence
- Generate: UI button, video scene, audio cue, 3D object
- Eval: Human review - do they feel like "same brand"?
- Measure: Semantic similarity scores, user perception tests

**Success Criteria**:
- âœ… 80%+ of reviewers identify artifacts as "same brand"
- âœ… Semantic similarity score > 0.75 across media pairs
- âœ… Zero conflicting translations (no contradictory manifestations)

**Status**: Phase 1 in progress (semantic metadata structure defined)

**Early Insights**:
- Personality axes (0-1 scales) work better than categorical labels
- Some semantics translate universally (energy, warmth), others are culturally specific
- Need clear "avoidance" rules, not just "usage" rules

**Next Steps**:
- [ ] Implement semantic token editor in design system tool
- [ ] Create 5 test tokens with full cross-media mappings
- [ ] Generate proof-of-concept artifacts
- [ ] Run initial coherence evaluations

---

### H2: Derivation Rules Reduce Specification Burden

**Hypothesis**: Instead of manually specifying every variant (hover, active, disabled, focus, etc.), we can define derivation rules that generate variants programmatically from base tokens.

**Rationale**:
- Traditional design systems document 40+ button states manually
- Most variants follow predictable patterns (hover = lighter, active = darker)
- Rules encode design intent better than examples
- Enables infinite contextual variants without documentation explosion

**Experiment Design**:

**Phase 1**: Define rule syntax
```typescript
{
  "button-primary": {
    "base": { "bg": "brand-primary", "color": "white" },
    "derivation": {
      "hover": {
        "bg": "lighten(brand-primary, 10%)",
        "transform": "translateY(-1px)"
      },
      "active": {
        "bg": "darken(brand-primary, 8%)",
        "transform": "scale(0.95)"
      },
      "disabled": {
        "bg": "desaturate(brand-primary, 80%)",
        "opacity": 0.6,
        "cursor": "not-allowed"
      }
    }
  }
}
```

**Phase 2**: Build rule engine
- Parse derivation rules
- Generate CSS/Swift/Kotlin from rules
- Validate generated code compiles

**Phase 3**: Compare with manual specification
- Generate 100 component variants (rules-based)
- Compare to manually specified equivalents
- Measure: coverage, consistency, maintainability

**Success Criteria**:
- âœ… Rules generate 95%+ of common variants correctly
- âœ… 80% reduction in specification size
- âœ… Zero inconsistencies (rules enforce uniformity)

**Status**: Phase 1 complete (syntax defined), Phase 2 starting

**Early Insights**:
- Need both absolute rules (`bg: "color-primary"`) and relative rules (`lighten: 10%`)
- Some rules are universal (hover = lighter), others brand-specific
- Edge cases need explicit overrides (rules + exceptions pattern)

**Open Questions**:
- How do we handle conflicting rules? (Priority system? Designer override?)
- Can AI learn derivation patterns from examples?
- What's the right balance of rules vs. explicit specs?

**Next Steps**:
- [ ] Implement basic rule engine (color transformations)
- [ ] Generate button variants from rules
- [ ] Compare to manual Tailwind classes
- [ ] Measure specification reduction

---

### H3: AI Can Generate Brand-Aligned Content With Sufficient Context

**Hypothesis**: Given rich semantic tokens, creative direction rules, and example artifacts, AI (Claude, GPT-4, etc.) can generate production-quality, brand-aligned content across media types.

**Rationale**:
- Modern LLMs understand design principles
- They lack brand-specific context, not capability
- Experience System provides that context
- With proper evals, AI output can match human quality

**Experiment Design**:

**Phase 1**: Establish baseline (no ES context)
```
Prompt: "Create a hero section for a SaaS product"
Result: Generic, off-brand, requires heavy editing
```

**Phase 2**: Add ES context via Claude skills/rules
```
System Context: [Experience System JSON]
Skill: "Generate UI components using ES tokens"
Rule: "Never use hardcoded colors, always use semantic tokens"

Prompt: "Create a hero section for a SaaS product"
Result: [Measure brand alignment, token usage]
```

**Phase 3**: Multi-modal generation
```
Prompt: "Create a product launch campaign"
Expected: Hero section + video + audio, all cohesive
Measure: Cross-media coherence, brand alignment
```

**Success Criteria**:
- âœ… 80%+ token usage (vs. hardcoded values)
- âœ… Brand alignment score > 0.85
- âœ… 70%+ of AI output ships without human editing
- âœ… Cross-media coherence score > 0.75

**Status**: Phase 2 in progress (Claude skill syntax being designed)

**Early Insights**:
- AI understands brand personality axes very well
- Struggles with subtle brand rules without examples
- Works best with "show, don't tell" (examples > descriptions)
- Needs iterative refinement loop, not one-shot generation

**Open Questions**:
- How much context is optimal? (Too little = generic, too much = confused)
- Can AI learn from rejected outputs? (RLHF for brand)
- Should we fine-tune models on brand examples?

**Next Steps**:
- [ ] Create Claude skill for component generation
- [ ] Test with 20 component generation tasks
- [ ] Measure brand alignment and token usage
- [ ] Iterate on context structure

---

### H4: Automated Evals Can Replace Manual Brand Reviews

**Hypothesis**: With the right evaluation framework, we can automatically detect brand violations and quality issues, reducing or eliminating manual review burden.

**Rationale**:
- Manual review doesn't scale to AI-generated content
- Many brand rules are objectively measurable
- Automated evals enable rapid iteration
- Humans review exceptions, not everything

**Experiment Design**:

**Phase 1**: Define eval dimensions
```typescript
interface BrandEval {
  accessibility: {
    contrastRatio: number        // WCAG AA/AAA
    colorBlindSafe: boolean
    minTouchTarget: boolean
  }

  brandAlignment: {
    colorUsage: number           // Are colors used per rules?
    personalityMatch: number     // Matches brand personality?
    toneConsistency: number      // Copy matches voice?
  }

  technicalQuality: {
    codeQuality: number
    performance: number
    validity: boolean
  }
}
```

**Phase 2**: Implement evaluators
- Contrast ratio calculator
- Color usage analyzer
- Personality scoring (ML model)
- Code linter integration

**Phase 3**: Validate against human reviews
- Generate 100 artifacts
- Run automated evals
- Get human review scores
- Measure correlation (do evals match humans?)

**Success Criteria**:
- âœ… 90%+ correlation with human reviewers on objective metrics
- âœ… 80%+ correlation on subjective metrics (brand alignment)
- âœ… Zero false negatives (never approve bad content)
- âœ… < 20% false positives (don't reject good content unnecessarily)

**Status**: Phase 1 complete (eval dimensions defined)

**Early Insights**:
- Objective metrics (accessibility, code quality) are straightforward
- Subjective metrics (brand alignment, tone) need ML models
- Need clear thresholds (what score = "pass"?)
- Different eval standards for different artifact types

**Open Questions**:
- Can we train a "brand alignment" model from curated examples?
- How do we handle edge cases that violate rules but "feel right"?
- What's the cost-accuracy tradeoff for eval complexity?

**Next Steps**:
- [ ] Build accessibility evaluator
- [ ] Build color usage analyzer
- [ ] Test on existing components
- [ ] Measure human-eval correlation

---

### H5: Experience Systems Can Self-Improve Through Usage Data

**Hypothesis**: By tracking how ES-generated content performs (usage metrics, A/B tests, user feedback), the system can suggest improvements to tokens, rules, and creative direction.

**Rationale**:
- Traditional design systems stagnate between redesigns
- Usage data reveals what works in practice
- Automated analysis can identify patterns humans miss
- Continuous improvement > periodic audits

**Experiment Design**:

**Phase 1**: Instrument tracking
```typescript
track({
  artifact: 'hero-section-variant-a',
  events: {
    impressions: 10000,
    clicks: 500,        // 5% CTR
    bounceRate: 0.4,
    timeOnPage: 45
  },
  metadata: {
    primaryColor: 'brand-primary',
    ctaSize: 'large',
    motionStyle: 'subtle'
  }
})
```

**Phase 2**: Analyze patterns
```typescript
// Example insight
const analysis = {
  finding: "CTAs with 'large' size have 23% higher click rate",
  confidence: 0.87,
  recommendation: "Update button sizing rules to default to 'large'",
  impact: "+115 conversions/month estimated"
}
```

**Phase 3**: Automated improvements
```typescript
// System proposes change
const proposal = {
  change: "Update button.derivation.size default from 'medium' to 'large'",
  rationale: "23% CTR improvement across 50 A/B tests",
  breakingChange: false,
  autoApprove: false  // Requires human review
}
```

**Success Criteria**:
- âœ… System identifies 5+ actionable insights per month
- âœ… 80%+ of proposed changes improve metrics
- âœ… Zero degradations slip through (safeguards work)
- âœ… Time-to-improvement < 1 week (vs. months for redesign)

**Status**: Phase 1 design in progress (tracking schema)

**Early Insights**:
- Need both quantitative (CTR) and qualitative (user feedback) data
- Correlation â‰  causation (careful with suggestions)
- Small sample sizes = high variance (need significance testing)

**Open Questions**:
- How do we attribute outcomes to specific ES decisions?
- What metrics matter most? (Conversions? Engagement? Brand perception?)
- How do we handle conflicting data? (One test says X, another says Y)

**Next Steps**:
- [ ] Define tracking schema
- [ ] Implement basic analytics
- [ ] Run pilot A/B test
- [ ] Build insight detection engine

---

## Validated Hypotheses {#validated}

### V1: OKLCH Color Space Enables Better Palette Generation

**Hypothesis**: Using OKLCH instead of RGB/HSL for color generation produces more perceptually uniform and aesthetically pleasing palettes.

**Experiment**: Built palette generator with both RGB and OKLCH. Generated 100 palettes, got designer feedback.

**Results**:
- âœ… OKLCH palettes rated 4.2/5 vs RGB 2.8/5
- âœ… Better contrast ratios (OKLCH maintains perceptual lightness)
- âœ… More predictable color relationships

**Conclusion**: **VALIDATED**. OKLCH is superior for algorithmic palette generation.

**Implementation**: Palette generator uses OKLCH exclusively (lib/palette-generator.ts)

**Paper Trail**: 30+ commits in design system tool implementing OKLCH picker

---

### V2: Live Preview Accelerates Design Iteration

**Hypothesis**: Designers can iterate faster with real-time preview than with export-reload cycle.

**Experiment**: Built live preview system with 7 templates. Measured iteration speed.

**Results**:
- âœ… Iteration time: 5 seconds (live) vs 45 seconds (export-reload)
- âœ… 9x faster iteration
- âœ… Designers tested 3x more variants per session

**Conclusion**: **VALIDATED**. Live preview dramatically accelerates workflow.

**Implementation**: 7 preview templates with real-time CSS injection (components/studio/preview-templates.tsx)

---

### V3: Semantic Color Names Improve Developer Experience

**Hypothesis**: Developers prefer semantic names (`color-primary`) over descriptive names (`blue-600`).

**Experiment**: Code review analysis. Measured token usage patterns.

**Results**:
- âœ… Semantic tokens used correctly 94% of time
- âœ… Descriptive tokens misused 31% of time (blue-600 for non-brand uses)
- âœ… Semantic = clearer intent, fewer mistakes

**Conclusion**: **VALIDATED**. Semantic naming is better for developer UX and brand consistency.

**Implementation**: All tokens use semantic naming (primitives.colors.primary, not primitives.colors.blue)

---

## Invalidated Hypotheses {#invalidated}

### I1: Designers Will Manually Configure All Tokens

**Hypothesis**: Designers want full control and will manually set every token value.

**Experiment**: Built comprehensive token configurator. Observed usage.

**Results**:
- âŒ 80% of tokens left at default
- âŒ Most designers only adjusted primary color and font
- âŒ Full configurator was overwhelming

**Conclusion**: **INVALIDATED**. Designers want smart defaults + targeted control, not exhaustive configuration.

**Pivot**: Built palette generator (1-2 seed colors â†’ full palette). Much better adoption.

---

### I2: Brand Guidelines Can Be Fully Automated

**Hypothesis**: We can translate existing brand PDFs to executable rules automatically.

**Experiment**: Attempted to parse brand guidelines using GPT-4.

**Results**:
- âŒ 40% of guidelines too vague ("use blue sparingly")
- âŒ 25% contradictory ("trustworthy yet innovative")
- âŒ Many implicit rules not documented

**Conclusion**: **INVALIDATED**. Need human-in-loop to codify brand guidelines.

**Pivot**: Build guided workflow for translating guidelines to rules (Phase 4).

---

### I3: One Size Fits All Evaluation

**Hypothesis**: Same evaluation criteria work for all artifact types.

**Experiment**: Tried applying web accessibility evals to video content.

**Results**:
- âŒ Accessibility metrics don't translate (WCAG for UI â‰  video)
- âŒ Different media have different quality dimensions
- âŒ Need media-specific eval frameworks

**Conclusion**: **INVALIDATED**. Need specialized evaluators per media type.

**Pivot**: Building media-specific eval modules (Phase 4).

---

## Open Questions {#questions}

### Technical Questions

**Q1**: What's the optimal token granularity?
- Too few = not enough control
- Too many = overwhelming complexity
- Current: 17 semantic colors. Is this right?

**Q2**: How do we version Experience Systems?
- Semantic versioning (major.minor.patch)?
- When is a change "breaking"?
- How do we migrate between versions?

**Q3**: Can we detect unused tokens automatically?
- Static analysis of codebase
- Usage tracking in production
- Deprecation workflow

**Q4**: How do we handle platform-specific constraints?
- iOS doesn't support all CSS features
- Video has temporal dimension
- What's the abstraction layer?

### Design Questions

**Q5**: How much creative freedom should AI have?
- Full autonomy with guard rails?
- Guided generation with human approval?
- Collaborative iteration?

**Q6**: What makes a "good" semantic token?
- How specific? ("brand-primary" vs "cta-background")
- How many relationships? (Too connected = fragile)
- How stable? (Can we change it without breaking things?)

**Q7**: How do we balance consistency vs. creativity?
- Strict rules = boring but coherent
- Loose rules = creative but inconsistent
- Where's the sweet spot?

### Product Questions

**Q8**: Who is the primary user?
- Designers (configure system)?
- Developers (consume system)?
- AI agents (execute system)?
- All three?

**Q9**: What's the minimum viable ES?
- Colors + typography enough?
- Need motion + audio too?
- Platform coverage?

**Q10**: How do we measure success?
- Developer productivity?
- Brand consistency scores?
- Creative output volume?
- Cost savings?

---

## Research Timeline {#timeline}

### January 2025

**Week 1**: Foundation
- âœ… Built design system configurator prototype
- âœ… Implemented OKLCH palette generator
- âœ… Created live preview system

**Week 2**: Semantic Layer Exploration
- âœ… Defined semantic token structure
- âœ… Created cross-media mapping specification
- ðŸ”„ Building semantic token editor (in progress)

**Week 3**: Evaluation Framework
- ðŸ”„ Designing eval dimensions
- ðŸ”„ Building accessibility evaluator
- ðŸ“… Brand alignment prototype (planned)

**Week 4**: Generative Adapters
- ðŸ“… Midjourney adapter prototype (planned)
- ðŸ“… Claude skill integration (planned)

### February 2025 (Planned)

**Week 1-2**: Agent Integration
- Claude skills for component generation
- Rule-based validation
- Multi-step workflows

**Week 3-4**: Proof of Concept
- Generate complete landing page (multi-modal)
- Measure coherence across media
- User testing with designers

### March 2025 (Planned)

**Week 1-2**: Refinement
- Iterate based on POC feedback
- Improve evals and generators
- Optimize token structure

**Week 3-4**: Documentation & Communication
- Update whitepaper with learnings
- Create case studies
- Prepare demo for stakeholders

---

## Experiment Log Entries

### Entry 001: OKLCH Palette Generator
**Date**: 2025-01-20
**Hypothesis**: OKLCH enables better algorithmic palette generation than RGB/HSL
**Experiment**: Built dual generator, compared outputs
**Result**: VALIDATED - OKLCH superior for perceptual uniformity
**Artifacts**: `lib/palette-generator.ts`, 30+ commits
**Next**: Use OKLCH for all color derivation rules

---

### Entry 002: Semantic Token Structure
**Date**: 2025-01-21
**Hypothesis**: Rich metadata enables cross-media generation
**Experiment**: Designed token schema with semantic fields
**Result**: IN PROGRESS - Structure defined, testing pending
**Artifacts**: `docs/design-system-principles.md`
**Next**: Implement editor, create test tokens

---

### Entry 003: Live Preview Performance
**Date**: 2025-01-19
**Hypothesis**: Live preview accelerates iteration
**Experiment**: Measured time-to-iterate with/without preview
**Result**: VALIDATED - 9x faster iteration with live preview
**Artifacts**: `components/studio/preview-templates.tsx`
**Next**: Add more preview templates (video, audio)

---

### Entry 004: AI Generation Context Size
**Date**: 2025-01-21
**Hypothesis**: More context = better AI generation
**Experiment**: TBD - Will test varying context sizes
**Result**: NOT STARTED
**Next**: Define experiment protocol, run tests

---

### Entry 005: Brand Alignment Scoring
**Date**: 2025-01-21
**Hypothesis**: ML model can score brand alignment
**Experiment**: TBD - Need training data first
**Result**: NOT STARTED
**Next**: Collect on-brand/off-brand examples

---

## Research Methodology

### How We Conduct Experiments

1. **Formulate Hypothesis**: Clear, testable claim
2. **Design Experiment**: Methodology, success criteria
3. **Build Prototype**: Minimum code to test hypothesis
4. **Collect Data**: Quantitative metrics + qualitative feedback
5. **Analyze Results**: Statistical significance, patterns
6. **Document Learning**: What worked, what didn't, why
7. **Iterate or Pivot**: Continue, adjust, or abandon

### Our Standards

- **Falsifiable**: Hypotheses can be proven wrong
- **Measurable**: Clear success criteria
- **Reproducible**: Others can verify results
- **Transparent**: Share failures, not just successes
- **Iterative**: Learn fast, fail fast, improve fast

---

## Key Insights Accumulated

### 1. Semantic Beats Syntactic
**Finding**: Tokens with semantic meaning (role, personality) work better than purely visual specs (hex codes).
**Evidence**: Cross-media mapping requires semantic understanding
**Implication**: All tokens should have semantic metadata

### 2. Rules > Examples
**Finding**: Derivation rules (hover = +10% lightness) beat documented examples (40 button variants).
**Evidence**: Rules enable infinite variants, examples don't scale
**Implication**: Invest in rule engine, not exhaustive documentation

### 3. AI Needs Brand Context
**Finding**: Generic prompts produce generic output. ES context produces brand-aligned output.
**Evidence**: Early Claude skill tests show dramatic improvement with context
**Implication**: Context injection is critical, not optional

### 4. Evals Enable Speed
**Finding**: Automated evals let us iterate rapidly without manual review bottleneck.
**Evidence**: 9x faster iteration with live preview + validation
**Implication**: Invest heavily in evaluation framework

### 5. Designers Want Smart Defaults
**Finding**: Full control is overwhelming. Smart defaults + targeted adjustments preferred.
**Evidence**: Palette generator (minimal input, smart output) > full configurator
**Implication**: Provide sensible defaults, progressive disclosure

---

## Failures & Learnings

### Failure 1: Overcomplicated Token Structure
**What**: Initial token schema had 50+ fields per token
**Why It Failed**: Too complex, nobody used most fields
**Learning**: Start simple, add complexity only when needed
**Fix**: Reduced to 10 core fields, 20 optional

### Failure 2: Tried to Automate Brand Guideline Parsing
**What**: GPT-4 to parse brand PDFs â†’ executable rules
**Why It Failed**: Guidelines too vague, contradictory, or implicit
**Learning**: Human expertise required for brand codification
**Fix**: Build guided workflow for designer-led rule creation

### Failure 3: Universal Evaluation Criteria
**What**: Same evals for web, video, audio
**Why It Failed**: Different media have different quality dimensions
**Learning**: Need specialized evaluators per medium
**Fix**: Building media-specific eval modules

---

## Next Research Priorities

### Immediate (Next 2 Weeks)
1. **Semantic token editor**: Build UI for creating rich tokens
2. **Rule engine prototype**: Implement color derivation rules
3. **Basic evals**: Accessibility + color usage analyzers

### Short-term (Next Month)
4. **Midjourney adapter**: Proof-of-concept image generation
5. **Claude skills**: Component generation with ES context
6. **Cross-media coherence**: Measure semantic alignment

### Medium-term (Next Quarter)
7. **Multi-modal POC**: Complete campaign (web + video + audio)
8. **Usage analytics**: Track how ES is used in practice
9. **Self-improvement**: System suggests optimizations

---

## Research Team Notes

### Current Focus Areas

**Person A**: Semantic token design, cross-media mappings
**Person B**: Evaluation framework, brand alignment scoring
**Person C**: Generative adapters, AI integration
**Person D**: Deployment pipeline, governance

### Collaboration Touchpoints

- **Weekly sync**: Share progress, align priorities
- **Monthly review**: Assess hypotheses, plan experiments
- **Quarterly retrospective**: Big picture, strategic direction

### Open Collaboration

This research is shared openly. We welcome:
- Critique of hypotheses
- Suggestions for experiments
- Collaboration on validation
- Alternative approaches

**Contact**: [Research team contact]

---

## Appendix: Experimental Protocols

### Protocol A: Cross-Media Coherence Testing
1. Generate artifacts across 3+ media from same semantic token
2. Survey 20+ participants: "Do these feel like same brand?"
3. Measure: % agreement, confidence scores
4. Threshold: 80% agreement = coherent

### Protocol B: AI Generation Quality
1. Give Claude ES context + generation task
2. Generate 10 artifacts
3. Measure: token usage %, brand alignment score, edit distance
4. Threshold: 80% token usage, 0.85 alignment, <20% edits

### Protocol C: Derivation Rule Coverage
1. Define rules for component (e.g., button)
2. Generate variants (hover, active, disabled, etc.)
3. Compare to manually-specified equivalents
4. Measure: % coverage, consistency, maintainability

---

**Last Updated**: 2025-01-21
**Status**: Active Research
**Next Review**: 2025-02-01

This is a living document. Updated as we learn.
